"""
ê°œì„ ëœ í—¥í†  AI ìë™ì°¨ ë¶„ë¥˜ ì¶”ë¡  íŒŒì´í”„ë¼ì¸
- Hydra ì˜ì¡´ì„± ì œê±°, config.py í†µí•©
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  TTA êµ¬í˜„
- ìŠ¤íŠ¸ë¦¬ë° ë°°ì¹˜ ì¶”ë¡ 
- ì•ˆì „í•œ ê²°ê³¼ ì €ì¥
- ì„±ëŠ¥ ìµœì í™”
"""

import os
import sys
import gc
import time
import traceback
from typing import Optional, Dict, List, Tuple, Generator
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm
import albumentations as A
from albumentations.pytorch import ToTensorV2

# ë¡œì»¬ ëª¨ë“ˆ import
try:
    from src.utils.config import Config, get_config
    from src.models.backbone_factory import create_model, get_supported_models
    from src.data.data import CarDataset, get_dataloader
    from src.utils.utils import (
        get_memory_usage, clear_memory, safe_operation, retry_on_failure,
        initialize_environment
    )
    from src.utils.logger import logger
except ImportError as e:
    logger.error(f"âŒ ëª¨ë“ˆ Import ì‹¤íŒ¨: {str(e)}")
    logger.error("   ê°œì„ ëœ ëª¨ë“ˆë“¤ì´ í•„ìš”í•©ë‹ˆë‹¤")
    sys.exit(1)


class InferenceManager:
    """ì¶”ë¡  ê´€ë¦¬ í´ë˜ìŠ¤ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  ì•ˆì „í•œ ì¶”ë¡ """
    
    def __init__(self, config: Config):
        """
        Args:
            config: ì„¤ì • ê°ì²´
        """
        self.config = config
        self.device = self._setup_device()
        self.model = None
        self.class_to_idx = None
        self.idx_to_class = None
        
        # ì„±ëŠ¥ í†µê³„
        self.inference_stats = {
            'total_images': 0,
            'processing_time': 0.0,
            'avg_time_per_image': 0.0,
            'memory_peak': 0.0,
            'tta_times_used': 0
        }
        
        logger.info(f"ğŸ”® ì¶”ë¡  ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"   ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            logger.info(f"   ğŸ® GPU: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
    
    def _setup_device(self) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if self.config.inference.force_cpu:
            device = torch.device("cpu")
            logger.info("ğŸ’» ê°•ì œ CPU ëª¨ë“œ")
        elif self.config.system.device == "auto":
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            device = torch.device(self.config.system.device)
        
        return device
    
    @safe_operation("ëª¨ë¸ ë¡œë“œ")
    def load_model(self) -> bool:
        """ëª¨ë¸ ë° í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ"""
        
        # ëª¨ë¸ ê²½ë¡œ í™•ì¸
        if not self.config.inference.model_path:
            logger.error("âŒ ëª¨ë¸ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False
        
        if not os.path.exists(self.config.inference.model_path):
            logger.error(f"âŒ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {self.config.inference.model_path}")
            return False
        
        with memory_monitor("ëª¨ë¸ ë¡œë”©"):
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì„¤ì • ë¡œë“œ ì‹œë„
            try:
                checkpoint = torch.load(self.config.inference.model_path, map_location='cpu')
                if isinstance(checkpoint, dict) and 'config' in checkpoint:
                    saved_config = checkpoint['config']
                    # ëª¨ë¸ ê´€ë ¨ ì„¤ì • ì—…ë°ì´íŠ¸
                    self.config.model.backbone = saved_config.model.backbone
                    self.config.model.num_classes = saved_config.model.num_classes
                    logger.info("âœ… ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ")
            except Exception as e:
                logger.warning(f"âš ï¸  ì²´í¬í¬ì¸íŠ¸ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                logger.info("   ê¸°ë³¸ ì„¤ì • ì‚¬ìš©")
            
            # ëª¨ë¸ ìƒì„±
            self.model, actual_backbone = create_model(
                backbone=self.config.model.backbone,
                num_classes=self.config.model.num_classes,
                pretrained=False,  # ì¶”ë¡ ì‹œì—ëŠ” pretrained=False
                device=self.device
            )
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            self.model = create_model(
                self.model,
                self.config.inference.model_path,
                strict=False,  # ì¶”ë¡ ì‹œì—ëŠ” ê´€ëŒ€í•˜ê²Œ
                device=self.device
            )
            
            if self.model is None:
                logger.error("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
                return False
            
            # í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
            self.model.eval()
            
            # ëª¨ë¸ ì •ë³´
            total_params = sum(p.numel() for p in self.model.parameters())
            logger.info(f"ğŸ§  ëª¨ë¸ ë¡œë“œ ì™„ë£Œ:")
            logger.info(f"   ë°±ë³¸: {actual_backbone}")
            logger.info(f"   í´ë˜ìŠ¤ ìˆ˜: {self.config.model.num_classes}")
            logger.info(f"   íŒŒë¼ë¯¸í„°: {total_params:,}")
        
        # í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ
        self._load_class_mapping()
        
        return True
    
    def _load_class_mapping(self):
        """í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ"""
        
        # í´ë˜ìŠ¤ ë§¤í•‘ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
        class_mapping_paths = [
            getattr(self.config.inference, 'class_mapping_path', None),
            os.path.join(os.path.dirname(self.config.inference.model_path), "class_to_idx.json"),
            os.path.join(self.config.train.save_dir, "class_to_idx.json"),
            "./class_to_idx.json"
        ]
        
        class_mapping_path = None
        for path in class_mapping_paths:
            if path and os.path.exists(path):
                class_mapping_path = path
                break
        
        if class_mapping_path:
            self.class_to_idx = safe_json_load(class_mapping_path)
            if self.class_to_idx:
                self.idx_to_class = {v: k for k, v in self.class_to_idx.items()}
                logger.info(f"ğŸ“‚ í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ: {class_mapping_path}")
                logger.info(f"   í´ë˜ìŠ¤ ìˆ˜: {len(self.class_to_idx)}")
            else:
                logger.warning(f"âš ï¸  í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {class_mapping_path}")
        else:
            logger.warning("âš ï¸  í´ë˜ìŠ¤ ë§¤í•‘ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            logger.info("   í´ë˜ìŠ¤ëª…ì„ 'class_0', 'class_1', ... í˜•ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤")
        
        # í´ë˜ìŠ¤ëª… ìƒì„± (ë§¤í•‘ì´ ì—†ëŠ” ê²½ìš°)
        if not self.class_to_idx:
            self.class_names = [f'class_{i}' for i in range(self.config.model.num_classes)]
        else:
            self.class_names = [self.idx_to_class.get(i, f'class_{i}') 
                               for i in range(self.config.model.num_classes)]
    
    def get_tta_transforms(self) -> List[A.Compose]:
        """TTA ë³€í™˜ ë¦¬ìŠ¤íŠ¸ ìƒì„±"""
        try:
            return get_tta_augmentations()
        except:
            # Fallback TTA ë³€í™˜ë“¤
            logger.warning("âš ï¸  TTA ë³€í™˜ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ë³€í™˜ ì‚¬ìš©")
            return [
                # ì›ë³¸
                A.Compose([
                    A.Resize(224, 224),
                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                    ToTensorV2()
                ]),
                # ìˆ˜í‰ ë’¤ì§‘ê¸°
                A.Compose([
                    A.Resize(224, 224),
                    A.HorizontalFlip(p=1.0),
                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                    ToTensorV2()
                ]),
                # ì‘ì€ íšŒì „
                A.Compose([
                    A.Resize(224, 224),
                    A.Rotate(limit=10, p=1.0),
                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                    ToTensorV2()
                ])
            ]
    
    @memory_cleanup_decorator
    def predict_batch(self, images: torch.Tensor) -> torch.Tensor:
        """ë°°ì¹˜ ë‹¨ìœ„ ì˜ˆì¸¡"""
        images = images.to(self.device, non_blocking=True)
        
        with torch.no_grad():
            # Mixed Precision ì¶”ë¡  (ê°€ëŠ¥í•œ ê²½ìš°)
            if self.config.system.mixed_precision and self.device.type == "cuda":
                with torch.cuda.amp.autocast():
                    outputs = self.model(images)
            else:
                outputs = self.model(images)
            
            # ì†Œí”„íŠ¸ë§¥ìŠ¤ ì ìš©
            probabilities = F.softmax(outputs, dim=1)
        
        return probabilities.cpu()
    
    def predict_with_tta_streaming(self, dataloader: DataLoader, 
                                  tta_times: int = 5) -> Generator[Tuple[np.ndarray, List[str]], None, None]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ TTA ì˜ˆì¸¡ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì """
        
        if tta_times <= 1:
            # TTA ì—†ì´ ì¼ë°˜ ì¶”ë¡ 
            logger.info("âš¡ ì¼ë°˜ ì¶”ë¡  ëª¨ë“œ")
            
            for batch_images, batch_ids in tqdm(dataloader, desc="ì¶”ë¡  ì§„í–‰"):
                probabilities = self.predict_batch(batch_images)
                yield probabilities.numpy(), list(batch_ids)
                
                # ë°°ì¹˜ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
                del probabilities
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
        else:
            # TTA ì¶”ë¡ 
            logger.info(f"ğŸ”„ TTA ì¶”ë¡  ëª¨ë“œ (ë³€í™˜: {tta_times}ê°€ì§€)")
            
            # TTA ë³€í™˜ë“¤ ê°€ì ¸ì˜¤ê¸°
            tta_transforms = self.get_tta_transforms()
            selected_transforms = tta_transforms[:min(tta_times, len(tta_transforms))]
            
            self.inference_stats['tta_times_used'] = len(selected_transforms)
            
            # ì›ë³¸ ë°ì´í„°ì…‹ì˜ transform ë°±ì—…
            original_transform = dataloader.dataset.transform
            
            try:
                # ê° ë°°ì¹˜ì— ëŒ€í•´ TTA ìˆ˜í–‰
                for batch_idx, (_, batch_ids) in enumerate(tqdm(dataloader, desc="TTA ì¶”ë¡ ")):
                    
                    batch_tta_results = []
                    
                    # TTA ë³€í™˜ë³„ë¡œ ì˜ˆì¸¡
                    for tta_idx, tta_transform in enumerate(selected_transforms):
                        # ë°ì´í„°ì…‹ì˜ transform ë³€ê²½
                        dataloader.dataset.transform = tta_transform
                        
                        # í˜„ì¬ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ë“¤ì„ ë‹¤ì‹œ ë¡œë“œ
                        batch_images = []
                        for item_idx in range(len(batch_ids)):
                            dataset_idx = batch_idx * dataloader.batch_size + item_idx
                            if dataset_idx < len(dataloader.dataset):
                                image, _ = dataloader.dataset[dataset_idx]
                                batch_images.append(image)
                        
                        if batch_images:
                            batch_tensor = torch.stack(batch_images)
                            probabilities = self.predict_batch(batch_tensor)
                            batch_tta_results.append(probabilities.numpy())
                            
                            # ë©”ëª¨ë¦¬ ì •ë¦¬
                            del batch_tensor, probabilities
                    
                    # TTA ê²°ê³¼ í‰ê· 
                    if batch_tta_results:
                        avg_probabilities = np.mean(batch_tta_results, axis=0)
                        yield avg_probabilities, list(batch_ids)
                    
                    # ë°°ì¹˜ë³„ ë©”ëª¨ë¦¬ ì •ë¦¬
                    del batch_tta_results
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    # ì£¼ê¸°ì  ê°•ì œ ë©”ëª¨ë¦¬ ì •ë¦¬
                    if batch_idx % 10 == 0:
                        clear_memory(verbose=False)
            
            finally:
                # ì›ë³¸ transform ë³µì›
                dataloader.dataset.transform = original_transform
    
    @safe_operation("ë°°ì¹˜ ì¶”ë¡ ")
    def inference_batch(self) -> bool:
        """ë©”ì¸ ë°°ì¹˜ ì¶”ë¡  í•¨ìˆ˜"""
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° í™•ì¸
        if not os.path.exists(self.config.inference.test_csv):
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ ì—†ìŒ: {self.config.inference.test_csv}")
            return False
        
        logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: {self.config.inference.test_csv}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
        with memory_monitor("í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±"):
            try:
                test_transform = get_test_augmentations()
            except:
                # Fallback ë³€í™˜
                test_transform = A.Compose([
                    A.Resize(224, 224),
                    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                    ToTensorV2()
                ])
            
            test_dataset = TestDataset(
                test_csv=self.config.inference.test_csv,
                transform=test_transform,
                cache_images=False  # ì¶”ë¡ ì‹œì—ëŠ” ìºì‹± ë¹„í™œì„±í™”
            )
            
            logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset):,}ì¥")
        
        # ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
        available_memory_gb = get_memory_usage().get('ram_used', 4000) / 1024
        if available_memory_gb > self.config.inference.max_memory_gb:
            adjusted_batch_size = max(16, self.config.inference.batch_size // 2)
            logger.warning(f"âš ï¸  ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {self.config.inference.batch_size} â†’ {adjusted_batch_size}")
            self.config.inference.batch_size = adjusted_batch_size
        
        # ë°ì´í„°ë¡œë” ìƒì„±
        test_loader = get_dataloader(
            test_dataset,
            batch_size=self.config.inference.batch_size,
            shuffle=False,
            num_workers=self.config.inference.num_workers,
            memory_efficient=True,
            memory_limit_mb=2048
        )
        
        logger.info(f"ğŸ“¦ ë°ì´í„°ë¡œë” ì¤€ë¹„:")
        logger.info(f"   ë°°ì¹˜ í¬ê¸°: {self.config.inference.batch_size}")
        logger.info(f"   ì´ ë°°ì¹˜ ìˆ˜: {len(test_loader)}")
        logger.info(f"   TTA: {'Yes' if self.config.inference.use_tta else 'No'}")
        
        # ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ì¤€ë¹„
        all_predictions = []
        all_ids = []
        
        # ì¶”ë¡  ì‹œì‘
        start_time = time.time()
        
        try:
            tta_times = self.config.inference.tta_times if self.config.inference.use_tta else 1
            
            with memory_monitor("ì „ì²´ ì¶”ë¡ "):
                for batch_predictions, batch_ids in self.predict_with_tta_streaming(test_loader, tta_times):
                    all_predictions.append(batch_predictions)
                    all_ids.extend(batch_ids)
                    
                    # ì§„í–‰ ìƒí™© ë¡œê¹…
                    processed = len(all_ids)
                    if processed % (self.config.inference.batch_size * 10) == 0:
                        logger.info(f"   ì§„í–‰: {processed:,}/{len(test_dataset):,} ({processed/len(test_dataset)*100:.1f}%)")
            
            # ê²°ê³¼ ê²°í•©
            logger.info("ğŸ“Š ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")
            
            if all_predictions:
                final_predictions = np.vstack(all_predictions)
                
                # ê²°ê³¼ ê²€ì¦
                if len(all_ids) != len(final_predictions):
                    logger.error(f"âŒ ì˜ˆì¸¡ ê²°ê³¼ ë¶ˆì¼ì¹˜: IDs {len(all_ids)} vs Predictions {len(final_predictions)}")
                    return False
                
                # í™•ë¥  í•© ê²€ì¦
                prob_sums = np.sum(final_predictions, axis=1)
                if not np.allclose(prob_sums, 1.0, atol=1e-3):
                    logger.warning("âš ï¸  ì¼ë¶€ ì˜ˆì¸¡ì˜ í™•ë¥  í•©ì´ 1.0ì´ ì•„ë‹™ë‹ˆë‹¤")
                    # ì •ê·œí™”
                    final_predictions = final_predictions / prob_sums[:, np.newaxis]
                    logger.info("   í™•ë¥  ì •ê·œí™” ì™„ë£Œ")
                
                # ì œì¶œ íŒŒì¼ ìƒì„±
                submission = pd.DataFrame(final_predictions, columns=self.class_names)
                submission.insert(0, 'ID', all_ids)
                
                # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„± ë° ì €ì¥
                os.makedirs(os.path.dirname(self.config.inference.output_path), exist_ok=True)
                submission.to_csv(self.config.inference.output_path, index=False)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                processing_time = time.time() - start_time
                self.inference_stats.update({
                    'total_images': len(all_ids),
                    'processing_time': processing_time,
                    'avg_time_per_image': processing_time / len(all_ids),
                    'memory_peak': get_memory_usage().get('ram_used', 0)
                })
                
                # ê²°ê³¼ ì¶œë ¥
                logger.info(f"âœ… ì¶”ë¡  ì™„ë£Œ!")
                logger.info(f"   ğŸ“Š ì²˜ë¦¬ëœ ì´ë¯¸ì§€: {len(all_ids):,}ì¥")
                logger.info(f"   ğŸ“Š ì˜ˆì¸¡ shape: {final_predictions.shape}")
                logger.info(f"   ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {self.config.inference.output_path}")
                logger.info(f"   â±ï¸  ì´ ì‹œê°„: {processing_time:.1f}ì´ˆ")
                logger.info(f"   âš¡ ì´ë¯¸ì§€ë‹¹ í‰ê· : {self.inference_stats['avg_time_per_image']*1000:.1f}ms")
                
                if self.config.inference.use_tta:
                    logger.info(f"   ğŸ”„ TTA ë³€í™˜: {self.inference_stats['tta_times_used']}ê°€ì§€")
                
                # ìƒ˜í”Œ ì˜ˆì¸¡ ì¶œë ¥ (ìƒìœ„ 3ê°œ í´ë˜ìŠ¤)
                self._show_sample_predictions(final_predictions, all_ids)
                
                return True
            else:
                logger.error("âŒ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
        
        except Exception as e:
            logger.error(f"âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {str(e)}")
            logger.error(traceback.format_exc())
            return False
        
        finally:
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            clear_memory(verbose=True)
    
    def _show_sample_predictions(self, predictions: np.ndarray, ids: List[str], num_samples: int = 5):
        """ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥"""
        logger.info(f"\nğŸ“‹ ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ (ìƒìœ„ {num_samples}ê°œ):")
        logger.info("-" * 60)
        
        for i in range(min(num_samples, len(predictions))):
            sample_pred = predictions[i]
            sample_id = ids[i]
            
            # ìƒìœ„ 3ê°œ í´ë˜ìŠ¤
            top3_indices = np.argsort(sample_pred)[-3:][::-1]
            top3_probs = sample_pred[top3_indices]
            top3_classes = [self.class_names[idx] for idx in top3_indices]
            
            logger.info(f"ID: {sample_id}")
            for j, (cls, prob) in enumerate(zip(top3_classes, top3_probs)):
                logger.info(f"   {j+1}. {cls}: {prob:.4f}")
            logger.info("")
    
    @safe_operation("ëª¨ë¸ ìµœì í™”")
    def optimize_model_for_inference(self):
        """ì¶”ë¡ ìš© ëª¨ë¸ ìµœì í™”"""
        if self.model is None:
            return
        
        try:
            # TorchScript ë³€í™˜ ì‹œë„
            logger.info("âš¡ ëª¨ë¸ ìµœì í™” ì‹œë„ ì¤‘...")
            
            dummy_input = torch.randn(1, 3, 224, 224).to(self.device)
            
            with torch.no_grad():
                # íŠ¸ë ˆì´ìŠ¤ ê°€ëŠ¥ì„± í™•ì¸
                traced_model = torch.jit.trace(self.model, dummy_input)
                
                # ìµœì í™” ì ìš©
                optimized_model = torch.jit.optimize_for_inference(traced_model)
                
                # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
                test_output = optimized_model(dummy_input)
                
                # ì„±ê³µì‹œ ëª¨ë¸ êµì²´
                self.model = optimized_model
                logger.info("âœ… ëª¨ë¸ ìµœì í™” ì™„ë£Œ (TorchScript)")
                
        except Exception as e:
            logger.warning(f"âš ï¸  ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {str(e)}")
            logger.info("   ì›ë³¸ ëª¨ë¸ ì‚¬ìš©")


@retry_on_failure(max_retries=2, delay=1.0)
def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # í™˜ê²½ ì´ˆê¸°í™”
    initialize_environment(seed=42, deterministic=True)
    
    # ì„¤ì • ë¡œë“œ
    try:
        # ëª…ë ¹í–‰ ì¸ìë¡œ ì„¤ì • íƒ€ì… ì§€ì • ê°€ëŠ¥
        config_type = "default"
        if len(sys.argv) > 1:
            config_type = sys.argv[1]
        
        config = get_config(config_type)
        
        # ì¶”ë¡  ê´€ë ¨ ì„¤ì • ê²€ì¦
        if not config.inference.model_path:
            logger.error("âŒ ëª¨ë¸ ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            logger.info("   config.inference.model_pathë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return
        
        if not config.inference.test_csv:
            logger.error("âŒ í…ŒìŠ¤íŠ¸ CSV ê²½ë¡œê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            logger.info("   config.inference.test_csvë¥¼ ì„¤ì •í•˜ì„¸ìš”")
            return
        
        logger.info("ğŸ”® ì¶”ë¡  ì„¤ì •:")
        logger.info(f"   ëª¨ë¸: {config.inference.model_path}")
        logger.info(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {config.inference.test_csv}")
        logger.info(f"   ì¶œë ¥: {config.inference.output_path}")
        logger.info(f"   ë°°ì¹˜ í¬ê¸°: {config.inference.batch_size}")
        logger.info(f"   TTA: {config.inference.use_tta}")
        
        if not config.validate():
            logger.error("âŒ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨")
            return
        
    except Exception as e:
        logger.error(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return
    
    # ì¶”ë¡  ì‹œì‘
    try:
        inference_manager = InferenceManager(config)
        
        # ëª¨ë¸ ë¡œë“œ
        if not inference_manager.load_model():
            logger.error("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # ëª¨ë¸ ìµœì í™” (ì„ íƒì )
        if getattr(config.inference, 'optimize_model', False):
            inference_manager.optimize_model_for_inference()
        
        # ì¶”ë¡  ì‹¤í–‰
        success = inference_manager.inference_batch()
        
        if success:
            logger.info("ğŸ‰ ì¶”ë¡  ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
            logger.info(f"ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: ì•™ìƒë¸” ì ìš© (ensemble_v2.py)")
        else:
            logger.error("âŒ ì¶”ë¡  ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¶”ë¡  ì¤‘ë‹¨")
        
    except Exception as e:
        logger.error(f"âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
    
    finally:
        # ìµœì¢… ì •ë¦¬
        clear_memory(verbose=True)
        logger.info("ğŸ§¹ ì¶”ë¡  ì„¸ì…˜ ì •ë¦¬ ì™„ë£Œ")


if __name__ == "__main__":
    main()