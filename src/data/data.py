"""
ê°œì„ ëœ ë°ì´í„° ì²˜ë¦¬ ëª¨ë“ˆ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„°ë¡œë”©
- utils.py í†µí•© ë° ì•ˆì „í•œ ì´ë¯¸ì§€ ì²˜ë¦¬
- ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ ì§€ì›
"""

import os
import gc
import time
from typing import List, Tuple, Optional, Union, Callable
from pathlib import Path
from collections import Counter, defaultdict
import warnings

import torch
from torch.utils.data import Dataset, DataLoader, Subset, Sampler
import numpy as np
import pandas as pd
import cv2
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import StratifiedKFold

# ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ import
import logging
logger = logging.getLogger(__name__)

# í•„ìš”í•œ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ ì •ì˜í•˜ê±°ë‚˜ ê°„ë‹¨í•œ ë²„ì „ ì‚¬ìš©
def safe_image_load(image_path, target_size=None):
    """ê°„ë‹¨í•œ ì´ë¯¸ì§€ ë¡œë“œ"""
    import cv2
    image = cv2.imread(image_path)
    if image is not None:
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return image

def memory_monitor(name):
    """ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„° (context manager)"""
    class SimpleMonitor:
        def __enter__(self):
            logger.debug(f"ì‹œì‘: {name}")
            return self
        def __exit__(self, *args):
            logger.debug(f"ì™„ë£Œ: {name}")
    return SimpleMonitor()

def safe_operation(name):
    """ê°„ë‹¨í•œ ì•ˆì „ ì‘ì—… ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        return func
    return decorator

def memory_cleanup_decorator(func):
    """ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ì •ë¦¬ ë°ì½”ë ˆì´í„°"""
    return func

def validate_image_dataset(image_dir, min_size=32, max_aspect_ratio=10.0):
    """ê°„ë‹¨í•œ ë°ì´í„°ì…‹ ê²€ì¦"""
    return {'errors': []}

def get_memory_usage():
    """ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
    import psutil
    return {
        'ram_used': psutil.virtual_memory().used / 1024 / 1024,  # MB
        'ram_available': psutil.virtual_memory().available / 1024 / 1024  # MB
    }

def clear_memory(verbose=False):
    """ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    import gc
    import torch
    
    # Python ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    gc.collect()
    
    # PyTorch CUDA ìºì‹œ ì •ë¦¬ (GPU ì‚¬ìš©ì‹œ)
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    if verbose:
        memory_info = get_memory_usage()
        logger.info(f"ğŸ§¹ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ: RAM {memory_info['ram_used']:.1f}MB ì‚¬ìš© ì¤‘")

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore')


class MemoryEfficientCarDataset(Dataset):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìë™ì°¨ ë¶„ë¥˜ ë°ì´í„°ì…‹"""
    
    def __init__(self, root_dir: str = None, csv_path: str = None, 
                 transform=None, is_train: bool = True,
                 cache_images: bool = False, max_cache_size: int = 1000,
                 lazy_loading: bool = True):
        """
        Args:
            root_dir: ì´ë¯¸ì§€ê°€ ì €ì¥ëœ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
            csv_path: CSV íŒŒì¼ ê²½ë¡œ (ìˆìœ¼ë©´ CSV ìš°ì„  ì‚¬ìš©)
            transform: ì´ë¯¸ì§€ ë³€í™˜
            is_train: í›ˆë ¨ìš© ì—¬ë¶€
            cache_images: ì´ë¯¸ì§€ ìºì‹± ì—¬ë¶€
            max_cache_size: ìµœëŒ€ ìºì‹œ í¬ê¸°
            lazy_loading: ì§€ì—° ë¡œë”© ì—¬ë¶€
        """
        self.root_dir = root_dir
        self.transform = transform
        self.is_train = is_train
        self.cache_images = cache_images
        self.max_cache_size = max_cache_size
        self.lazy_loading = lazy_loading
        
        # ì´ë¯¸ì§€ ìºì‹œ
        self._image_cache = {} if cache_images else None
        self._cache_hits = 0
        self._cache_misses = 0
        
        # ë°ì´í„° ë¡œë“œ
        self._load_data(csv_path, root_dir)
        
        # ë°ì´í„°ì…‹ ê²€ì¦ (ì„ íƒì )
        if not lazy_loading:
            self._validate_dataset()
    
    def _load_data(self, csv_path: str, root_dir: str):
        """ë°ì´í„° ë¡œë“œ"""
        if csv_path and os.path.exists(csv_path):
            # CSV íŒŒì¼ì—ì„œ ë¡œë“œ
            self.df = pd.read_csv(csv_path)
            self.use_csv = True
            self.image_paths = self.df['img_path'].tolist()
            self.labels = self.df['label'].tolist() if 'label' in self.df.columns else None
            logger.info(f"âœ… CSVì—ì„œ ë°ì´í„° ë¡œë“œ: {len(self.image_paths):,}ì¥")
        else:
            # í´ë” êµ¬ì¡°ì—ì„œ ë°ì´í„° ë¡œë“œ
            with memory_monitor("í´ë” ìŠ¤ìº”"):
                self.image_paths, self.labels, self.class_to_idx = self._scan_folders_optimized(root_dir)
            self.use_csv = False
            logger.info(f"âœ… í´ë”ì—ì„œ ë°ì´í„° ë¡œë“œ: {len(self.image_paths):,}ì¥, {len(self.class_to_idx)}ê°œ í´ë˜ìŠ¤")
    
    def _scan_folders_optimized(self, root_dir: str) -> Tuple[List[str], List[str], dict]:
        """ìµœì í™”ëœ í´ë” ìŠ¤ìº”"""
        image_paths = []
        labels = []
        
        # í´ë˜ìŠ¤ í´ë”ë“¤ ìŠ¤ìº”
        class_folders = sorted([f for f in os.listdir(root_dir) 
                               if os.path.isdir(os.path.join(root_dir, f))])
        
        # í´ë˜ìŠ¤ëª… -> ì¸ë±ìŠ¤ ë§¤í•‘
        class_to_idx = {class_name: idx for idx, class_name in enumerate(class_folders)}
        
        # ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ê°œì„ 
        total_files = 0
        valid_files = 0
        
        for class_name in class_folders:
            class_path = os.path.join(root_dir, class_name)
            
            # ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì°¾ê¸° (í™•ì¥ì ì²´í¬ ìµœì í™”)
            image_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}
            image_files = [
                f for f in os.listdir(class_path) 
                if any(f.lower().endswith(ext) for ext in image_extensions)
            ]
            
            for image_file in image_files:
                image_path = os.path.join(class_path, image_file)
                total_files += 1
                
                # ë¹ ë¥¸ ê²€ì¦ (lazy loadingì¸ ê²½ìš° ê¸°ë³¸ ì²´í¬ë§Œ)
                if self.lazy_loading or self._quick_validate_image(image_path):
                    image_paths.append(image_path)
                    labels.append(class_name)
                    valid_files += 1
        
        logger.info(f"ğŸ“Š ìŠ¤ìº” ê²°ê³¼: {valid_files:,}/{total_files:,} ìœ íš¨ ({valid_files/total_files*100:.1f}%)")
        return image_paths, labels, class_to_idx
    
    def _quick_validate_image(self, image_path: str) -> bool:
        """ë¹ ë¥¸ ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            # íŒŒì¼ í¬ê¸° ì²´í¬ (ë§¤ìš° ë¹ ë¦„)
            file_size = os.path.getsize(image_path)
            if file_size < 1024:  # 1KB ë¯¸ë§Œì€ ì˜ì‹¬ìŠ¤ëŸ¬ì›€
                return False
            
            # íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ ì²´í¬ (ì„ íƒì )
            with open(image_path, 'rb') as f:
                header = f.read(8)
                # JPEG, PNG ì‹œê·¸ë‹ˆì²˜ ì²´í¬
                if header.startswith(b'\xff\xd8') or header.startswith(b'\x89PNG'):
                    return True
            
            return True  # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
            
        except Exception:
            return False
    
    def _validate_dataset(self):
        """ì „ì²´ ë°ì´í„°ì…‹ ê²€ì¦ (lazy_loading=Falseì¸ ê²½ìš°)"""
        logger.info("ğŸ” ë°ì´í„°ì…‹ ê²€ì¦ ì¤‘...")
        
        with memory_monitor("ë°ì´í„°ì…‹ ê²€ì¦"):
            validation_result = validate_image_dataset(
                image_dir=self.root_dir if not self.use_csv else None,
                min_size=32,
                max_aspect_ratio=10.0
            )
        
        if validation_result['errors']:
            logger.warning(f"âš ï¸  ê²€ì¦ ì˜¤ë¥˜: {len(validation_result['errors'])}ê°œ")
            for error in validation_result['errors'][:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                logger.warning(f"   {error}")
    
    @memory_cleanup_decorator
    def __getitem__(self, idx):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì•„ì´í…œ ë¡œë“œ"""
        image_path = self.image_paths[idx]
        
        # ìºì‹œì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
        if self.cache_images and image_path in self._image_cache:
            image = self._image_cache[image_path].copy()
            self._cache_hits += 1
        else:
            # ì•ˆì „í•œ ì´ë¯¸ì§€ ë¡œë“œ (utils.py ì‚¬ìš©)
            image = safe_image_load(image_path, target_size=None)
            self._cache_misses += 1
            
            # ìºì‹œì— ì €ì¥ (ìš©ëŸ‰ ì œí•œ)
            if self.cache_images and len(self._image_cache) < self.max_cache_size:
                self._image_cache[image_path] = image.copy()
        
        # ë³€í™˜ ì ìš©
        if self.transform:
            try:
                if isinstance(self.transform, A.Compose):
                    # Albumentations
                    transformed = self.transform(image=image)
                    image = transformed['image']
                else:
                    # torchvision transforms
                    image = self.transform(image)
            except Exception as e:
                logger.warning(f"âš ï¸  ë³€í™˜ ì‹¤íŒ¨: {image_path} - {str(e)}")
                # ê¸°ë³¸ ë³€í™˜ ì ìš©
                image = self._get_default_transform()(image=image)['image']
        else:
            # ê¸°ë³¸ ë³€í™˜ ì ìš©
            image = self._get_default_transform()(image=image)['image']
        
        if self.is_train and self.labels:
            # í›ˆë ¨ìš©: ì´ë¯¸ì§€ + ë¼ë²¨
            if self.use_csv:
                label = self.labels[idx]  # ì´ë¯¸ ìˆ«ì ì¸ë±ìŠ¤
            else:
                label_name = self.labels[idx]
                label = self.class_to_idx[label_name]  # ë¬¸ìì—´ -> ìˆ«ì ë³€í™˜
            
            return image, torch.tensor(label, dtype=torch.long)
        else:
            # í…ŒìŠ¤íŠ¸ìš©: ì´ë¯¸ì§€ë§Œ
            return image, torch.tensor(0, dtype=torch.long)  # ë”ë¯¸ ë¼ë²¨
    
    def _get_default_transform(self):
        """ê¸°ë³¸ ë³€í™˜"""
        return A.Compose([
            A.Resize(224, 224),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    
    def __len__(self):
        return len(self.image_paths)
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„"""
        if not self.cache_images:
            return {"cache_enabled": False}
        
        total_requests = self._cache_hits + self._cache_misses
        hit_rate = self._cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            "cache_enabled": True,
            "cache_size": len(self._image_cache),
            "max_cache_size": self.max_cache_size,
            "cache_hits": self._cache_hits,
            "cache_misses": self._cache_misses,
            "hit_rate": hit_rate
        }
    
    def clear_cache(self):
        """ìºì‹œ ë¹„ìš°ê¸°"""
        if self.cache_images:
            self._image_cache.clear()
            self._cache_hits = 0
            self._cache_misses = 0
            clear_memory(verbose=False)
    
    def get_class_distribution(self) -> dict:
        """í´ë˜ìŠ¤ ë¶„í¬ ë°˜í™˜"""
        if self.labels:
            return Counter(self.labels)
        return {}
    
    def get_class_weights(self) -> torch.Tensor:
        """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)"""
        if not self.labels:
            return None
        
        class_counts = Counter(self.labels)
        total_samples = len(self.labels)
        num_classes = len(class_counts)
        
        # ì—­ë¹ˆë„ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = []
        for class_name in sorted(class_counts.keys()):
            weight = total_samples / (num_classes * class_counts[class_name])
            weights.append(weight)
        
        return torch.FloatTensor(weights)


class StreamingCarDataset(Dataset):
    """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹"""
    
    def __init__(self, file_list_path: str, transform=None, 
                 chunk_size: int = 1000, preload_next_chunk: bool = True):
        """
        Args:
            file_list_path: íŒŒì¼ ëª©ë¡ ê²½ë¡œ (ê° ì¤„ì— ì´ë¯¸ì§€ ê²½ë¡œ)
            transform: ì´ë¯¸ì§€ ë³€í™˜
            chunk_size: ì²­í¬ í¬ê¸°
            preload_next_chunk: ë‹¤ìŒ ì²­í¬ ë¯¸ë¦¬ ë¡œë“œ ì—¬ë¶€
        """
        self.file_list_path = file_list_path
        self.transform = transform
        self.chunk_size = chunk_size
        self.preload_next_chunk = preload_next_chunk
        
        # íŒŒì¼ ëª©ë¡ ë¡œë“œ
        with open(file_list_path, 'r') as f:
            self.file_paths = [line.strip() for line in f.readlines()]
        
        self.total_samples = len(self.file_paths)
        self.current_chunk = 0
        self.chunk_data = {}
        
        # ì²« ë²ˆì§¸ ì²­í¬ ë¡œë“œ
        self._load_chunk(0)
        
        logger.info(f"ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ ì´ˆê¸°í™”: {self.total_samples:,}ê°œ íŒŒì¼")
    
    def _load_chunk(self, chunk_idx: int):
        """ì²­í¬ ë¡œë“œ"""
        start_idx = chunk_idx * self.chunk_size
        end_idx = min(start_idx + self.chunk_size, self.total_samples)
        
        if chunk_idx in self.chunk_data:
            return  # ì´ë¯¸ ë¡œë“œë¨
        
        chunk_files = self.file_paths[start_idx:end_idx]
        chunk_images = []
        
        logger.info(f"ğŸ“¦ ì²­í¬ {chunk_idx} ë¡œë“œ ì¤‘: {len(chunk_files)}ê°œ íŒŒì¼")
        
        for file_path in chunk_files:
            image = safe_image_load(file_path)
            chunk_images.append(image)
        
        self.chunk_data[chunk_idx] = chunk_images
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬: ì˜¤ë˜ëœ ì²­í¬ ì œê±°
        if len(self.chunk_data) > 3:  # ìµœëŒ€ 3ê°œ ì²­í¬ë§Œ ìœ ì§€
            oldest_chunk = min(self.chunk_data.keys())
            if oldest_chunk != chunk_idx:
                del self.chunk_data[oldest_chunk]
                clear_memory(verbose=False)
    
    def __getitem__(self, idx):
        chunk_idx = idx // self.chunk_size
        local_idx = idx % self.chunk_size
        
        # í•„ìš”í•œ ì²­í¬ ë¡œë“œ
        if chunk_idx not in self.chunk_data:
            self._load_chunk(chunk_idx)
        
        # ë‹¤ìŒ ì²­í¬ ë¯¸ë¦¬ ë¡œë“œ (ì„ íƒì )
        if self.preload_next_chunk and (chunk_idx + 1) not in self.chunk_data:
            next_chunk_start = (chunk_idx + 1) * self.chunk_size
            if next_chunk_start < self.total_samples:
                self._load_chunk(chunk_idx + 1)
        
        # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        image = self.chunk_data[chunk_idx][local_idx]
        
        # ë³€í™˜ ì ìš©
        if self.transform:
            if isinstance(self.transform, A.Compose):
                transformed = self.transform(image=image)
                image = transformed['image']
            else:
                image = self.transform(image)
        
        return image, torch.tensor(0, dtype=torch.long)  # ë”ë¯¸ ë¼ë²¨
    
    def __len__(self):
        return self.total_samples


class TestDataset(Dataset):
    """ê°œì„ ëœ í…ŒìŠ¤íŠ¸ ì „ìš© ë°ì´í„°ì…‹"""
    
    def __init__(self, test_csv: str, transform=None, cache_images: bool = False):
        """
        Args:
            test_csv: í…ŒìŠ¤íŠ¸ CSV íŒŒì¼ ê²½ë¡œ
            transform: ì´ë¯¸ì§€ ë³€í™˜
            cache_images: ì´ë¯¸ì§€ ìºì‹± ì—¬ë¶€ (í…ŒìŠ¤íŠ¸ìš©ì€ ì¼ë°˜ì ìœ¼ë¡œ False)
        """
        self.df = pd.read_csv(test_csv)
        self.transform = transform if transform else self._get_default_transform()
        self.cache_images = cache_images
        self._image_cache = {} if cache_images else None
        
        # ê²½ë¡œ ì •ê·œí™”
        self.df['img_path'] = self.df['img_path'].apply(self._normalize_path)
        
        logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¡œë“œ: {len(self.df):,}ì¥")
    
    def _normalize_path(self, img_path: str) -> str:
        """ê²½ë¡œ ì •ê·œí™” - ë‹¨ìˆœí™”ëœ ë²„ì „"""
        img_path = img_path.strip()
        
        # ì ˆëŒ€ ê²½ë¡œì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if os.path.isabs(img_path) and os.path.exists(img_path):
            return img_path
        
        # ìƒëŒ€ ê²½ë¡œ ì²˜ë¦¬
        possible_paths = [
            img_path,
            os.path.join('data', img_path.lstrip('./')),
            img_path.replace('\\', '/')  # Windows ê²½ë¡œ êµ¬ë¶„ì ë³€í™˜
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        # ëª¨ë“  ê²½ë¡œê°€ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë°˜í™˜ (ì—ëŸ¬ëŠ” ë¡œë”© ì‹œ ì²˜ë¦¬)
        return img_path
    
    def _get_default_transform(self):
        """í…ŒìŠ¤íŠ¸ìš© ê¸°ë³¸ ë³€í™˜"""
        return A.Compose([
            A.Resize(224, 224),
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
    
    @memory_cleanup_decorator
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img_path = row['img_path']
        img_id = row['ID']
        
        # ìºì‹œì—ì„œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
        if self.cache_images and img_path in self._image_cache:
            image = self._image_cache[img_path].copy()
        else:
            # ì•ˆì „í•œ ì´ë¯¸ì§€ ë¡œë“œ
            image = safe_image_load(img_path)
            
            # ìºì‹œì— ì €ì¥
            if self.cache_images:
                self._image_cache[img_path] = image.copy()
        
        # ë³€í™˜ ì ìš©
        if self.transform:
            try:
                transformed = self.transform(image=image)
                image = transformed['image']
            except Exception as e:
                logger.warning(f"âš ï¸  í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨: {img_path} - {str(e)}")
                # ê¸°ë³¸ ë³€í™˜ìœ¼ë¡œ ì¬ì‹œë„
                default_transform = self._get_default_transform()
                transformed = default_transform(image=image)
                image = transformed['image']
        
        return image, img_id
    
    def __len__(self):
        return len(self.df)


class MemoryEfficientDataLoader:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ë¡œë” ë˜í¼"""
    
    def __init__(self, dataset: Dataset, batch_size: int = 32, 
                 shuffle: bool = True, num_workers: int = 4,
                 memory_limit_mb: float = 2048, **kwargs):
        """
        Args:
            dataset: ë°ì´í„°ì…‹
            batch_size: ë°°ì¹˜ í¬ê¸°
            shuffle: ì…”í”Œ ì—¬ë¶€
            num_workers: ì›Œì»¤ ìˆ˜
            memory_limit_mb: ë©”ëª¨ë¦¬ ì œí•œ (MB)
            **kwargs: DataLoaderì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì
        """
        self.dataset = dataset
        self.memory_limit_mb = memory_limit_mb
        self.initial_memory = get_memory_usage()
        
        # ë©”ëª¨ë¦¬ ì œí•œì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
        adjusted_batch_size = self._adjust_batch_size(batch_size)
        if adjusted_batch_size != batch_size:
            logger.warning(f"âš ï¸  ë©”ëª¨ë¦¬ ì œí•œìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° ì¡°ì •: {batch_size} â†’ {adjusted_batch_size}")
        
        # kwargsì—ì„œ ì¤‘ë³µ ì¸ì ì œê±° (ì¤‘ë³µ ë°©ì§€)
        kwargs_filtered = {k: v for k, v in kwargs.items() if k not in ['pin_memory', 'drop_last']}
        
        # DataLoader ìƒì„±
        self.dataloader = DataLoader(
            dataset=dataset,
            batch_size=adjusted_batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available(),
            drop_last=True if shuffle else False,
            **kwargs_filtered
        )
        
        self._batch_count = 0
        self._memory_warnings = 0
        
    def _adjust_batch_size(self, requested_batch_size: int) -> int:
        """ë©”ëª¨ë¦¬ ì œí•œì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •"""
        available_memory = self.memory_limit_mb
        
        # ëŒ€ëµì ì¸ ë°°ì¹˜ë‹¹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (MB)
        # ì´ë¯¸ì§€ í¬ê¸°: 224x224x3, float32 = ~600KB per image
        estimated_memory_per_image = 0.6  # MB
        estimated_batch_memory = requested_batch_size * estimated_memory_per_image
        
        if estimated_batch_memory > available_memory * 0.8:  # 80% ì„ê³„ì¹˜
            adjusted_batch_size = max(1, int(available_memory * 0.8 / estimated_memory_per_image))
            return adjusted_batch_size
        
        return requested_batch_size
    
    def __iter__(self):
        for batch in self.dataloader:
            self._batch_count += 1
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì²´í¬
            if self._batch_count % 50 == 0:
                self._check_memory()
            
            yield batch
            
            # ë°°ì¹˜ ì²˜ë¦¬ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬ (í•„ìš”ì‹œ)
            if self._batch_count % 100 == 0:
                clear_memory(verbose=False)
    
    def _check_memory(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
        current_memory = get_memory_usage()
        memory_increase = current_memory['ram_used'] - self.initial_memory['ram_used']
        
        if memory_increase > self.memory_limit_mb:
            self._memory_warnings += 1
            logger.warning(f"âš ï¸  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {memory_increase:.1f}MB / {self.memory_limit_mb}MB")
            
            if self._memory_warnings >= 3:
                logger.error("âŒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì§€ì†ì ìœ¼ë¡œ ì´ˆê³¼ë©ë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ì„¸ìš”.")
    
    def __len__(self):
        return len(self.dataloader)


def get_kfold(dataset: Dataset, n_splits: int = 5, random_state: int = 42) -> StratifiedKFold:
    """í–¥ìƒëœ ì¸µí™” K-Fold êµì°¨ê²€ì¦ ìƒì„±"""
    if not hasattr(dataset, 'labels') or dataset.labels is None:
        raise ValueError("ë°ì´í„°ì…‹ì— ë¼ë²¨ì´ ì—†ìŠµë‹ˆë‹¤")
    
    # ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜
    if hasattr(dataset, 'class_to_idx'):
        labels = [dataset.class_to_idx[label] for label in dataset.labels]
    else:
        labels = dataset.labels
    
    # í´ë˜ìŠ¤ ë¶„í¬ ì²´í¬
    class_counts = Counter(labels)
    min_class_count = min(class_counts.values())
    
    if min_class_count < n_splits:
        logger.warning(f"âš ï¸  ì¼ë¶€ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜({min_class_count})ê°€ fold ìˆ˜({n_splits})ë³´ë‹¤ ì ìŠµë‹ˆë‹¤")
        n_splits = max(2, min_class_count)
        logger.warning(f"   fold ìˆ˜ë¥¼ {n_splits}ë¡œ ì¡°ì •í•©ë‹ˆë‹¤")
    
    # StratifiedKFold ìƒì„±
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    
    logger.info(f"ğŸ”€ {n_splits}-Fold ì¸µí™” êµì°¨ê²€ì¦ ìƒì„±")
    logger.info(f"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬: {dict(sorted(class_counts.items()))}")
    
    return skf.split(range(len(dataset)), labels)


@safe_operation("ë°ì´í„°ë¡œë” ìƒì„±")
def get_dataloader(dataset: Dataset, batch_size: int = 32, shuffle: bool = True, 
                  num_workers: int = 4, pin_memory: bool = True, 
                  collate_fn: Optional[Callable] = None,
                  memory_efficient: bool = True,
                  memory_limit_mb: float = 2048) -> Union[DataLoader, MemoryEfficientDataLoader]:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„°ë¡œë” ìƒì„±"""
    
    # ë©”ëª¨ë¦¬ ê¸°ë°˜ num_workers ì¡°ì •
    available_memory = get_memory_usage()['ram_used']
    if available_memory > 8000:  # 8GB ì´ìƒ
        max_workers = min(num_workers, 8)
    elif available_memory > 4000:  # 4GB ì´ìƒ
        max_workers = min(num_workers, 4)
    else:  # 4GB ë¯¸ë§Œ
        max_workers = min(num_workers, 2)
    
    if max_workers != num_workers:
        logger.info(f"ğŸ”§ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì›Œì»¤ ìˆ˜ ì¡°ì •: {num_workers} â†’ {max_workers}")
        num_workers = max_workers
    
    # CUDA ì‚¬ìš© ê°€ëŠ¥ì‹œì—ë§Œ pin_memory í™œì„±í™”
    if pin_memory and not torch.cuda.is_available():
        pin_memory = False
        logger.info("ğŸ’¾ CUDA ë¯¸ì‚¬ìš©ìœ¼ë¡œ pin_memory ë¹„í™œì„±í™”")
    
    if memory_efficient:
        return MemoryEfficientDataLoader(
            dataset=dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            memory_limit_mb=memory_limit_mb,
            collate_fn=collate_fn,
            pin_memory=pin_memory,
            drop_last=True if shuffle else False
        )
    else:
        return DataLoader(
            dataset=dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=pin_memory,
            collate_fn=collate_fn,
            drop_last=True if shuffle else False
        )


# CutMix/MixUp êµ¬í˜„ - ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€
@memory_cleanup_decorator
def cutmix_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CutMix ë°ì´í„° ì¦ê°•"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    y_a, y_b = y, y[index]
    
    # ì˜ë¼ë‚¼ ì˜ì—­ ê³„ì‚°
    _, _, H, W = x.shape
    cut_rat = np.sqrt(1. - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)
    
    # ì¤‘ì‹¬ì  ëœë¤ ì„ íƒ
    cx = np.random.randint(W)
    cy = np.random.randint(H)
    
    # ê²½ê³„ ì¡°ì •
    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)
    
    # CutMix ì ìš© (in-place ì—°ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
    
    # ì‹¤ì œ ë¹„ìœ¨ ê³„ì‚°
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
    
    return x, y_a, y_b, lam


@memory_cleanup_decorator
def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ MixUp ë°ì´í„° ì¦ê°•"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)
    
    # in-place ì—°ì‚°ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    mixed_x = x.clone()  # ë³µì‚¬ë³¸ ìƒì„±
    mixed_x.mul_(lam).add_(x[index], alpha=(1 - lam))
    
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam


def cutmix_collate_fn(batch):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ CutMixìš© collate í•¨ìˆ˜"""
    try:
        images, labels = zip(*batch)
        images = torch.stack(images)
        labels = torch.stack(labels)
        
        # CutMix ì ìš©
        images, labels_a, labels_b, lam = cutmix_data(images, labels, alpha=1.0)
        
        return images, (labels_a, labels_b, lam)
    except Exception as e:
        logger.warning(f"âš ï¸  CutMix collate ì˜¤ë¥˜: {str(e)}")
        # fallback: ì¼ë°˜ collate
        images, labels = zip(*batch)
        return torch.stack(images), torch.stack(labels)


def mixup_collate_fn(batch):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ MixUpìš© collate í•¨ìˆ˜"""
    try:
        images, labels = zip(*batch)
        images = torch.stack(images)
        labels = torch.stack(labels)
        
        # MixUp ì ìš©
        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=1.0)
        
        return images, (labels_a, labels_b, lam)
    except Exception as e:
        logger.warning(f"âš ï¸  MixUp collate ì˜¤ë¥˜: {str(e)}")
        # fallback: ì¼ë°˜ collate
        images, labels = zip(*batch)
        return torch.stack(images), torch.stack(labels)


class ImbalancedDatasetSampler(torch.utils.data.Sampler):
    """ê°œì„ ëœ ë¶ˆê· í˜• ë°ì´í„°ì…‹ìš© ìƒ˜í”ŒëŸ¬"""
    
    def __init__(self, dataset: Dataset, num_samples: Optional[int] = None):
        self.dataset = dataset
        self.num_samples = num_samples or len(dataset)
        
        # í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°
        label_to_count = {}
        indices_per_class = defaultdict(list)
        
        for idx in range(len(dataset)):
            _, label = dataset[idx]
            if isinstance(label, torch.Tensor):
                label = label.item()
            
            label_to_count[label] = label_to_count.get(label, 0) + 1
            indices_per_class[label].append(idx)
        
        # ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = []
        for idx in range(len(dataset)):
            _, label = dataset[idx]
            if isinstance(label, torch.Tensor):
                label = label.item()
            weights.append(1.0 / label_to_count[label])
        
        self.weights = torch.DoubleTensor(weights)
        self.indices_per_class = dict(indices_per_class)
        
        logger.info(f"âš–ï¸  ë¶ˆê· í˜• ìƒ˜í”ŒëŸ¬ ì´ˆê¸°í™”: {len(label_to_count)}ê°œ í´ë˜ìŠ¤")
        for label, count in sorted(label_to_count.items()):
            logger.info(f"   í´ë˜ìŠ¤ {label}: {count:,}ê°œ")
    
    def __iter__(self):
        return iter(torch.multinomial(self.weights, self.num_samples, replacement=True))
    
    def __len__(self):
        return self.num_samples


def create_balanced_dataloader(dataset: Dataset, batch_size: int = 32, 
                             num_workers: int = 4, memory_efficient: bool = True) -> DataLoader:
    """ê· í˜• ì¡íŒ ë°ì´í„°ë¡œë” ìƒì„± (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)"""
    sampler = ImbalancedDatasetSampler(dataset)
    
    if memory_efficient:
        return MemoryEfficientDataLoader(
            dataset=dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=num_workers
        )
    else:
        return DataLoader(
            dataset=dataset,
            batch_size=batch_size,
            sampler=sampler,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )


def get_train_val_split(dataset: Dataset, val_ratio: float = 0.2, 
                       random_state: int = 42) -> Tuple[Subset, Subset]:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í• """
    from sklearn.model_selection import train_test_split
    
    indices = list(range(len(dataset)))
    labels = None
    
    # ë¼ë²¨ ì¶”ì¶œ
    if hasattr(dataset, 'labels') and dataset.labels:
        labels = dataset.labels
    
    with memory_monitor("ë°ì´í„° ë¶„í• "):
        if labels:
            # ì¸µí™” ë¶„í• 
            train_idx, val_idx = train_test_split(
                indices, 
                test_size=val_ratio, 
                stratify=labels,
                random_state=random_state
            )
        else:
            # ëœë¤ ë¶„í• 
            train_idx, val_idx = train_test_split(
                indices, 
                test_size=val_ratio, 
                random_state=random_state
            )
    
    train_subset = Subset(dataset, train_idx)
    val_subset = Subset(dataset, val_idx)
    
    logger.info(f"ğŸ”„ ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
    logger.info(f"   ğŸ“Š í›ˆë ¨: {len(train_subset):,}ì¥")
    logger.info(f"   ğŸ“Š ê²€ì¦: {len(val_subset):,}ì¥")
    
    return train_subset, val_subset


def analyze_dataset(dataset: Dataset) -> dict:
    """ë°ì´í„°ì…‹ ë¶„ì„ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë²„ì „"""
    analysis = {
        'total_samples': len(dataset),
        'num_classes': 0,
        'class_distribution': {},
    }
    
    # í´ë˜ìŠ¤ ì •ë³´
    if hasattr(dataset, 'class_to_idx'):
        analysis['num_classes'] = len(dataset.class_to_idx)
        analysis['class_distribution'] = dataset.get_class_distribution()
    
    # ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„ (ìƒ˜í”Œë§ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
    sample_size = min(100, len(dataset))
    sample_indices = np.random.choice(len(dataset), sample_size, replace=False)
    
    image_sizes = []
    valid_samples = 0
    
    logger.info(f"ğŸ” ë°ì´í„°ì…‹ ë¶„ì„ ì¤‘... (ìƒ˜í”Œ: {sample_size}ì¥)")
    
    for idx in sample_indices:
        try:
            if hasattr(dataset, 'image_paths'):
                image_path = dataset.image_paths[idx]
                with Image.open(image_path) as img:
                    image_sizes.append(img.size)
                    valid_samples += 1
        except Exception:
            continue
    
    if image_sizes:
        widths, heights = zip(*image_sizes)
        analysis.update({
            'avg_width': np.mean(widths),
            'avg_height': np.mean(heights),
            'min_width': min(widths),
            'min_height': min(heights),
            'max_width': max(widths),
            'max_height': max(heights),
            'valid_sample_rate': valid_samples / sample_size
        })
    
    return analysis


# í¸ì˜ í•¨ìˆ˜ë“¤
CarDataset = MemoryEfficientCarDataset  # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­

def save_class_mapping(class_to_idx: dict, save_path: str):
    """í´ë˜ìŠ¤ ë§¤í•‘ ì €ì¥"""
    from utils import safe_json_save
    safe_json_save(class_to_idx, save_path)
    logger.info(f"ğŸ’¾ í´ë˜ìŠ¤ ë§¤í•‘ ì €ì¥: {save_path}")


def load_class_mapping(load_path: str) -> dict:
    """í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ"""
    from utils import safe_json_load
    class_to_idx = safe_json_load(load_path)
    if class_to_idx:
        logger.info(f"ğŸ“‚ í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ: {load_path}")
        return class_to_idx
    else:
        raise ValueError(f"í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {load_path}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    from utils import initialize_environment
    
    initialize_environment()
    logger.info("ğŸ§ª ê°œì„ ëœ ë°ì´í„° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
    try:
        if os.path.exists("data/train"):
            with memory_monitor("ë°ì´í„°ì…‹ ìƒì„±"):
                dataset = MemoryEfficientCarDataset(
                    root_dir="data/train",
                    cache_images=True,
                    max_cache_size=100
                )
            
            logger.info(f"âœ… ë°ì´í„°ì…‹ ë¡œë“œ: {len(dataset):,}ì¥")
            
            # ìºì‹œ í…ŒìŠ¤íŠ¸
            _ = dataset[0]  # ìºì‹œ ë¯¸ìŠ¤
            _ = dataset[0]  # ìºì‹œ íˆíŠ¸
            cache_stats = dataset.get_cache_stats()
            logger.info(f"ğŸ“Š ìºì‹œ í†µê³„: {cache_stats}")
            
            # ë¶„ì„
            analysis = analyze_dataset(dataset)
            logger.info(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: {analysis}")
            
            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„°ë¡œë” í…ŒìŠ¤íŠ¸
            dataloader = get_dataloader(
                dataset, 
                batch_size=8, 
                memory_efficient=True,
                memory_limit_mb=1024
            )
            logger.info(f"âœ… ë°ì´í„°ë¡œë” ìƒì„±: {len(dataloader)}ë°°ì¹˜")
            
            # ì²« ë²ˆì§¸ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
            first_batch = next(iter(dataloader))
            logger.info(f"ğŸ“¦ ì²« ë°°ì¹˜ shape: {first_batch[0].shape}")
            
    except Exception as e:
        logger.error(f"âŒ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    logger.info("ğŸ‰ ê°œì„ ëœ ë°ì´í„° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

def create_submission(predictions, ids, class_names, output_path):
    """ì œì¶œ íŒŒì¼ ìƒì„±"""
    import pandas as pd
    
    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    submission = pd.DataFrame(predictions, columns=class_names)
    submission.insert(0, 'ID', ids)
    
    # CSV íŒŒì¼ë¡œ ì €ì¥
    submission.to_csv(output_path, index=False)
    
    return submission