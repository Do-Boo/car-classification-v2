#!/usr/bin/env python3
"""
ê°œì„ ëœ ë°ì´í„° ìœ í‹¸ë¦¬í‹°
UTF-8 ì¸ì½”ë”©, ê²½ë¡œ ì •ê·œí™”, ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„°ì…‹ í¬í•¨
"""

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import torch
from torch.utils.data import Dataset
from PIL import Image
import cv2
from sklearn.model_selection import StratifiedKFold

class EfficientCarDataset(Dataset):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìë™ì°¨ ë°ì´í„°ì…‹"""
    
    def __init__(self, 
                 image_paths: List[str], 
                 labels: List[int], 
                 transform=None,
                 cache_size: int = 1000):
        """
        Args:
            image_paths: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            labels: ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            transform: ì´ë¯¸ì§€ ë³€í™˜ í•¨ìˆ˜
            cache_size: ìºì‹œí•  ì´ë¯¸ì§€ ìˆ˜ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì ˆ)
        """
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.cache_size = cache_size
        self.cache = {}
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        image_path = self.image_paths[idx]
        label = self.labels[idx]
        
        # ìºì‹œì—ì„œ ì´ë¯¸ì§€ í™•ì¸
        if idx in self.cache:
            image = self.cache[idx]
        else:
            # ì´ë¯¸ì§€ ë¡œë“œ
            try:
                image = Image.open(image_path).convert('RGB')
                
                # ìºì‹œ í¬ê¸° ê´€ë¦¬
                if len(self.cache) < self.cache_size:
                    self.cache[idx] = image
                    
            except Exception as e:
                print(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path}, ì—ëŸ¬: {e}")
                # ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„± (ê²€ì€ìƒ‰ 224x224)
                image = Image.new('RGB', (224, 224), color=(0, 0, 0))
        
        # ë³€í™˜ ì ìš©
        if self.transform:
            image = self.transform(image)
            
        return image, label

def load_data_with_encoding(csv_path: str, encoding: str = 'utf-8') -> pd.DataFrame:
    """UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ CSV íŒŒì¼ ë¡œë“œ"""
    try:
        df = pd.read_csv(csv_path, encoding=encoding)
        print(f"âœ… CSV ë¡œë“œ ì„±ê³µ: {csv_path} (ì¸ì½”ë”©: {encoding})")
        return df
    except UnicodeDecodeError:
        # UTF-8 ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
        encodings = ['cp949', 'euc-kr', 'latin-1']
        for enc in encodings:
            try:
                df = pd.read_csv(csv_path, encoding=enc)
                print(f"âœ… CSV ë¡œë“œ ì„±ê³µ: {csv_path} (ì¸ì½”ë”©: {enc})")
                return df
            except UnicodeDecodeError:
                continue
        raise ValueError(f"CSV íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

def normalize_paths(base_path: str, relative_paths: List[str]) -> List[str]:
    """ê²½ë¡œ ì •ê·œí™”"""
    base_path = Path(base_path).resolve()
    normalized_paths = []
    
    for rel_path in relative_paths:
        # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        full_path = base_path / rel_path
        normalized_paths.append(str(full_path.resolve()))
    
    return normalized_paths

def apply_class_mapping(labels: List[str], class_mapping: Dict[str, int]) -> List[int]:
    """í´ë˜ìŠ¤ ì´ë¦„ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜"""
    mapped_labels = []
    missing_classes = set()
    
    for label in labels:
        if label in class_mapping:
            mapped_labels.append(class_mapping[label])
        else:
            missing_classes.add(label)
            # ê¸°ë³¸ê°’ìœ¼ë¡œ 0 í• ë‹¹ (ë˜ëŠ” ì—ëŸ¬ ë°œìƒ)
            mapped_labels.append(0)
    
    if missing_classes:
        print(f"âš ï¸ ë§¤í•‘ë˜ì§€ ì•Šì€ í´ë˜ìŠ¤ë“¤: {missing_classes}")
    
    return mapped_labels

def create_stratified_folds(labels: List[int], n_splits: int = 5, random_state: int = 42) -> List[Tuple[List[int], List[int]]]:
    """Stratified K-Fold ìƒì„±"""
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    
    folds = []
    for train_idx, val_idx in skf.split(range(len(labels)), labels):
        folds.append((train_idx.tolist(), val_idx.tolist()))
    
    print(f"âœ… {n_splits}-Fold êµì°¨ê²€ì¦ ìƒì„± ì™„ë£Œ")
    return folds

def validate_dataset(image_paths: List[str], labels: List[int]) -> Tuple[List[str], List[int]]:
    """ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬ ë° ì •ë¦¬"""
    valid_paths = []
    valid_labels = []
    invalid_count = 0
    
    for path, label in zip(image_paths, labels):
        if os.path.exists(path):
            try:
                # ì´ë¯¸ì§€ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
                with Image.open(path) as img:
                    img.verify()
                valid_paths.append(path)
                valid_labels.append(label)
            except Exception as e:
                print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€: {path}, ì—ëŸ¬: {e}")
                invalid_count += 1
        else:
            print(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {path}")
            invalid_count += 1
    
    print(f"âœ… ë°ì´í„°ì…‹ ê²€ì¦ ì™„ë£Œ: ìœ íš¨ {len(valid_paths)}ê°œ, ë¬´íš¨ {invalid_count}ê°œ")
    return valid_paths, valid_labels

def get_class_distribution(labels: List[int], class_names: Optional[List[str]] = None) -> Dict:
    """í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„"""
    unique, counts = np.unique(labels, return_counts=True)
    
    distribution = {}
    for class_idx, count in zip(unique, counts):
        class_name = class_names[class_idx] if class_names else f"Class_{class_idx}"
        distribution[class_name] = count
    
    # í†µê³„ ì •ë³´
    stats = {
        'total_samples': len(labels),
        'num_classes': len(unique),
        'min_samples': int(np.min(counts)),
        'max_samples': int(np.max(counts)),
        'mean_samples': float(np.mean(counts)),
        'std_samples': float(np.std(counts))
    }
    
    return {
        'distribution': distribution,
        'statistics': stats
    }

def save_data_info(data_info: Dict, save_path: str):
    """ë°ì´í„° ì •ë³´ ì €ì¥"""
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(data_info, f, ensure_ascii=False, indent=2)
    print(f"âœ… ë°ì´í„° ì •ë³´ ì €ì¥: {save_path}")

def load_and_prepare_data(train_csv_path: str, 
                         test_csv_path: str,
                         class_mapping_path: str,
                         data_root: str) -> Dict:
    """ì „ì²´ ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„"""
    
    print("ğŸ”„ ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„ ì‹œì‘...")
    
    # 1. CSV íŒŒì¼ ë¡œë“œ
    train_df = load_data_with_encoding(train_csv_path)
    test_df = load_data_with_encoding(test_csv_path)
    
    # 2. í´ë˜ìŠ¤ ë§¤í•‘ ë¡œë“œ
    with open(class_mapping_path, 'r', encoding='utf-8') as f:
        class_mapping = json.load(f)
    
    # 3. ê²½ë¡œ ì •ê·œí™”
    train_paths = normalize_paths(data_root, train_df['img_path'].tolist())
    test_paths = normalize_paths(data_root, test_df['img_path'].tolist())
    
    # 4. ë¼ë²¨ ë§¤í•‘
    train_labels = apply_class_mapping(train_df['label'].tolist(), class_mapping)
    
    # 5. ë°ì´í„°ì…‹ ìœ íš¨ì„± ê²€ì‚¬
    train_paths, train_labels = validate_dataset(train_paths, train_labels)
    test_paths, _ = validate_dataset(test_paths, [0] * len(test_paths))
    
    # 6. í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„
    class_names = list(class_mapping.keys())
    class_dist = get_class_distribution(train_labels, class_names)
    
    # 7. K-Fold ìƒì„±
    folds = create_stratified_folds(train_labels)
    
    # 8. ê²°ê³¼ ì •ë¦¬
    data_info = {
        'train_size': len(train_paths),
        'test_size': len(test_paths),
        'num_classes': len(class_mapping),
        'class_distribution': class_dist,
        'folds': folds
    }
    
    print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   - í›ˆë ¨ ë°ì´í„°: {len(train_paths)}ê°œ")
    print(f"   - í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_paths)}ê°œ") 
    print(f"   - í´ë˜ìŠ¤ ìˆ˜: {len(class_mapping)}ê°œ")
    
    return {
        'train_paths': train_paths,
        'train_labels': train_labels,
        'test_paths': test_paths,
        'class_mapping': class_mapping,
        'data_info': data_info
    } 