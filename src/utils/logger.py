"""
WandB ë¡œê¹… ìœ í‹¸ë¦¬í‹°
- ì‹¤í—˜ ì¶”ì  ë° ì‹œê°í™”
- ë©”íŠ¸ë¦­ ë¡œê¹…
- ì´ë¯¸ì§€ ë° íŒŒì¼ ì—…ë¡œë“œ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶”ì 
"""

import wandb
import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Optional, List, Union
import os
import json
from pathlib import Path
from omegaconf import DictConfig, OmegaConf
import warnings
from PIL import Image
import cv2

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings('ignore')

class WandBLogger:
    """WandB ë¡œê±° í´ë˜ìŠ¤"""
    
    def __init__(self, project_name: str, experiment_name: Optional[str] = None,
                 config: Optional[DictConfig] = None, tags: Optional[List[str]] = None,
                 notes: Optional[str] = None, offline: bool = False):
        """
        Args:
            project_name: WandB í”„ë¡œì íŠ¸ëª…
            experiment_name: ì‹¤í—˜ëª… (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            config: ì‹¤í—˜ ì„¤ì •
            tags: íƒœê·¸ ë¦¬ìŠ¤íŠ¸
            notes: ì‹¤í—˜ ë…¸íŠ¸
            offline: ì˜¤í”„ë¼ì¸ ëª¨ë“œ
        """
        self.project_name = project_name
        self.experiment_name = experiment_name
        self.offline = offline
        self.run = None
        self.initialized = False
        
        # WandB ì´ˆê¸°í™”
        try:
            # ì„¤ì • ë³€í™˜
            wandb_config = self._convert_config(config) if config else {}
            
            # WandB ì‹œì‘
            self.run = wandb.init(
                project=project_name,
                name=experiment_name,
                config=wandb_config,
                tags=tags,
                notes=notes,
                mode="offline" if offline else "online",
                reinit=True
            )
            
            self.initialized = True
            print(f"âœ… WandB ë¡œê±° ì´ˆê¸°í™” ì™„ë£Œ")
            print(f"   ğŸ“Š í”„ë¡œì íŠ¸: {project_name}")
            print(f"   ğŸ”¬ ì‹¤í—˜ëª…: {self.run.name}")
            print(f"   ğŸ”— URL: {self.run.url if not offline else 'Offline ëª¨ë“œ'}")
            
        except Exception as e:
            print(f"âš ï¸  WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            print("   ğŸ“ ë¡œì»¬ ë¡œê¹…ìœ¼ë¡œ ëŒ€ì²´ë©ë‹ˆë‹¤")
            self.initialized = False
    
    def _convert_config(self, config: DictConfig) -> Dict:
        """OmegaConf ì„¤ì •ì„ dictë¡œ ë³€í™˜"""
        if isinstance(config, DictConfig):
            return OmegaConf.to_container(config, resolve=True)
        return config
    
    def log_metrics(self, metrics: Dict[str, Union[float, int]], step: Optional[int] = None):
        """ë©”íŠ¸ë¦­ ë¡œê¹…"""
        if self.initialized and self.run:
            self.run.log(metrics, step=step)
        else:
            # ë¡œì»¬ ë¡œê¹…
            step_str = f"Step {step}: " if step is not None else ""
            metric_str = ", ".join([f"{k}={v:.4f}" for k, v in metrics.items()])
            print(f"ğŸ“Š {step_str}{metric_str}")
    
    def log_image(self, key: str, image: Union[np.ndarray, torch.Tensor, str], 
                  caption: Optional[str] = None, step: Optional[int] = None):
        """ì´ë¯¸ì§€ ë¡œê¹…"""
        if not self.initialized or not self.run:
            return
        
        try:
            # ì´ë¯¸ì§€ íƒ€ì…ë³„ ì²˜ë¦¬
            if isinstance(image, str):
                # íŒŒì¼ ê²½ë¡œ
                if os.path.exists(image):
                    wandb_image = wandb.Image(image, caption=caption)
                else:
                    print(f"âš ï¸  ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: {image}")
                    return
            elif isinstance(image, torch.Tensor):
                # PyTorch í…ì„œ
                if image.dim() == 4:  # ë°°ì¹˜ ì°¨ì› ì œê±°
                    image = image[0]
                if image.dim() == 3 and image.shape[0] in [1, 3]:  # CHW -> HWC
                    image = image.permute(1, 2, 0)
                image_np = image.detach().cpu().numpy()
                
                # ì •ê·œí™” (0-255 ë²”ìœ„ë¡œ)
                if image_np.max() <= 1.0:
                    image_np = (image_np * 255).astype(np.uint8)
                
                wandb_image = wandb.Image(image_np, caption=caption)
            else:
                # NumPy ë°°ì—´
                wandb_image = wandb.Image(image, caption=caption)
            
            self.run.log({key: wandb_image}, step=step)
            
        except Exception as e:
            print(f"âš ï¸  ì´ë¯¸ì§€ ë¡œê¹… ì‹¤íŒ¨: {str(e)}")
    
    def log_images(self, key: str, images: List[Union[np.ndarray, torch.Tensor]], 
                   captions: Optional[List[str]] = None, step: Optional[int] = None):
        """ë‹¤ì¤‘ ì´ë¯¸ì§€ ë¡œê¹…"""
        if not self.initialized or not self.run:
            return
        
        wandb_images = []
        for i, img in enumerate(images):
            caption = captions[i] if captions and i < len(captions) else f"Image {i+1}"
            
            try:
                if isinstance(img, torch.Tensor):
                    if img.dim() == 4:
                        img = img[0]
                    if img.dim() == 3 and img.shape[0] in [1, 3]:
                        img = img.permute(1, 2, 0)
                    img = img.detach().cpu().numpy()
                
                if img.max() <= 1.0:
                    img = (img * 255).astype(np.uint8)
                
                wandb_images.append(wandb.Image(img, caption=caption))
                
            except Exception as e:
                print(f"âš ï¸  ì´ë¯¸ì§€ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        if wandb_images:
            self.run.log({key: wandb_images}, step=step)
    
    def log_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray, 
                           class_names: Optional[List[str]] = None,
                           title: str = "Confusion Matrix", step: Optional[int] = None):
        """í˜¼ë™ í–‰ë ¬ ë¡œê¹…"""
        if not self.initialized or not self.run:
            return
        
        try:
            from sklearn.metrics import confusion_matrix
            
            cm = confusion_matrix(y_true, y_pred)
            
            # í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
            plt.figure(figsize=(10, 8))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                       xticklabels=class_names, yticklabels=class_names)
            plt.title(title)
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            plt.tight_layout()
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ë¡œê¹…
            temp_path = "temp_confusion_matrix.png"
            plt.savefig(temp_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            self.run.log({
                "confusion_matrix": wandb.Image(temp_path, caption=title)
            }, step=step)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.remove(temp_path)
                
        except Exception as e:
            print(f"âš ï¸  í˜¼ë™ í–‰ë ¬ ë¡œê¹… ì‹¤íŒ¨: {str(e)}")
    
    def log_learning_curve(self, train_losses: List[float], val_losses: List[float],
                          train_accs: List[float], val_accs: List[float],
                          title: str = "Learning Curves"):
        """í•™ìŠµ ê³¡ì„  ë¡œê¹…"""
        if not self.initialized or not self.run:
            return
        
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
            
            # ì†ì‹¤ ê³¡ì„ 
            epochs = range(1, len(train_losses) + 1)
            ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)
            ax1.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)
            ax1.set_title('Model Loss')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # ì •í™•ë„ ê³¡ì„ 
            ax2.plot(epochs, train_accs, 'b-', label='Train Accuracy', linewidth=2)
            ax2.plot(epochs, val_accs, 'r-', label='Validation Accuracy', linewidth=2)
            ax2.set_title('Model Accuracy')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Accuracy')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            plt.suptitle(title)
            plt.tight_layout()
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ë¡œê¹…
            temp_path = "temp_learning_curve.png"
            plt.savefig(temp_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            self.run.log({
                "learning_curves": wandb.Image(temp_path, caption=title)
            })
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(temp_path):
                os.remove(temp_path)
                
        except Exception as e:
            print(f"âš ï¸  í•™ìŠµ ê³¡ì„  ë¡œê¹… ì‹¤íŒ¨: {str(e)}")
    
    def log_model_architecture(self, model: torch.nn.Module, input_size: tuple = (3, 224, 224)):
        """ëª¨ë¸ êµ¬ì¡° ë¡œê¹…"""
        if not self.initialized or not self.run:
            return
        
        try:
            # ëª¨ë¸ ìš”ì•½
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            model_info = {
                "total_parameters": total_params,
                "trainable_parameters": trainable_params,
                "model_size_mb": total_params * 4 / (1024 * 1024),  # float32 ê¸°ì¤€
                "input_size": input_size
            }
            
            self.run.log({"model_info": model_info})
            
            # ëª¨ë¸ êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì €ì¥
            model_summary = str(model)
            with open("model_architecture.txt", "w") as f:
                f.write(f"Model Architecture\n")
                f.write("=" * 50 + "\n")
                f.write(f"Total Parameters: {total_params:,}\n")
                f.write(f"Trainable Parameters: {trainable_params:,}\n")
                f.write(f"Model Size: {model_info['model_size_mb']:.2f} MB\n")
                f.write("=" * 50 + "\n\n")
                f.write(model_summary)
            
            # íŒŒì¼ ì—…ë¡œë“œ
            self.run.upload_file("model_architecture.txt")
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists("model_architecture.txt"):
                os.remove("model_architecture.txt")
                
            print(f"ğŸ“Š ëª¨ë¸ ì •ë³´ ë¡œê¹… ì™„ë£Œ")
            print(f"   ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
            print(f"   ğŸ“Š í›ˆë ¨ ê°€ëŠ¥: {trainable_params:,}")
            
        except Exception as e:
            print(f"âš ï¸  ëª¨ë¸ êµ¬ì¡° ë¡œê¹… ì‹¤íŒ¨: {str(e)}")
    
    def log_hyperparameters(self, hyperparams: Dict[str, Any]):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…"""
        if self.initialized and self.run:
            self.run.config.update(hyperparams)
            print(f"âš™ï¸  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: {len(hyperparams)}ê°œ")
    
    def log_artifact(self, file_path: str, artifact_name: str, artifact_type: str = "model"):
        """ì•„í‹°íŒ©íŠ¸ ë¡œê¹… (ëª¨ë¸, ë°ì´í„°ì…‹ ë“±)"""
        if not self.initialized or not self.run:
            return
        
        try:
            artifact = wandb.Artifact(artifact_name, type=artifact_type)
            artifact.add_file(file_path)
            self.run.log_artifact(artifact)
            print(f"ğŸ“¦ ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ: {artifact_name}")
            
        except Exception as e:
            print(f"âš ï¸  ì•„í‹°íŒ©íŠ¸ ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    
    def log_system_info(self):
        """ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…"""
        if not self.initialized or not self.run:
            return
        
        try:
            import psutil
            import platform
            
            system_info = {
                "platform": platform.platform(),
                "python_version": platform.python_version(),
                "cpu_count": psutil.cpu_count(),
                "memory_gb": psutil.virtual_memory().total / (1024**3),
                "pytorch_version": torch.__version__,
            }
            
            # GPU ì •ë³´
            if torch.cuda.is_available():
                system_info.update({
                    "gpu_count": torch.cuda.device_count(),
                    "gpu_name": torch.cuda.get_device_name(0),
                    "gpu_memory_gb": torch.cuda.get_device_properties(0).total_memory / (1024**3)
                })
            
            self.run.log({"system_info": system_info})
            print(f"ğŸ’» ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹… ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸  ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹… ì‹¤íŒ¨: {str(e)}")
    
    def watch_model(self, model: torch.nn.Module, log_freq: int = 100):
        """ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì """
        if self.initialized and self.run:
            try:
                wandb.watch(model, log="all", log_freq=log_freq)
                print(f"ğŸ‘ï¸  ëª¨ë¸ ê·¸ë˜ë””ì–¸íŠ¸ ì¶”ì  ì‹œì‘ (log_freq={log_freq})")
            except Exception as e:
                print(f"âš ï¸  ëª¨ë¸ ì¶”ì  ì‹¤íŒ¨: {str(e)}")
    
    def finish(self):
        """WandB ì„¸ì…˜ ì¢…ë£Œ"""
        if self.initialized and self.run:
            self.run.finish()
            print(f"âœ… WandB ì„¸ì…˜ ì¢…ë£Œ")
            self.initialized = False

# ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
_global_logger: Optional[WandBLogger] = None

def init_wandb(project_name: str, config: Optional[DictConfig] = None,
               experiment_name: Optional[str] = None, tags: Optional[List[str]] = None,
               notes: Optional[str] = None, offline: bool = False) -> WandBLogger:
    """WandB ë¡œê±° ì´ˆê¸°í™” (ì „ì—­)"""
    global _global_logger
    
    _global_logger = WandBLogger(
        project_name=project_name,
        experiment_name=experiment_name,
        config=config,
        tags=tags,
        notes=notes,
        offline=offline
    )
    
    return _global_logger

def log_metrics(metrics: Dict[str, Union[float, int]], step: Optional[int] = None):
    """ë©”íŠ¸ë¦­ ë¡œê¹… (ì „ì—­ í•¨ìˆ˜)"""
    if _global_logger:
        _global_logger.log_metrics(metrics, step)
    else:
        print(f"âš ï¸  WandB ë¡œê±°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

def log_image(key: str, image: Union[np.ndarray, torch.Tensor, str], 
              caption: Optional[str] = None, step: Optional[int] = None):
    """ì´ë¯¸ì§€ ë¡œê¹… (ì „ì—­ í•¨ìˆ˜)"""
    if _global_logger:
        _global_logger.log_image(key, image, caption, step)

def log_model_info(model: torch.nn.Module, input_size: tuple = (3, 224, 224)):
    """ëª¨ë¸ ì •ë³´ ë¡œê¹… (ì „ì—­ í•¨ìˆ˜)"""
    if _global_logger:
        _global_logger.log_model_architecture(model, input_size)
        _global_logger.log_system_info()

def finish_wandb():
    """WandB ì„¸ì…˜ ì¢…ë£Œ (ì „ì—­ í•¨ìˆ˜)"""
    global _global_logger
    if _global_logger:
        _global_logger.finish()
        _global_logger = None

class LocalLogger:
    """WandB ëŒ€ì‹  ë¡œì»¬ ë¡œê¹…"""
    
    def __init__(self, log_dir: str = "./logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        self.metrics_file = self.log_dir / "metrics.jsonl"
        self.config_file = self.log_dir / "config.json"
        
        print(f"ğŸ“ ë¡œì»¬ ë¡œê±° ì´ˆê¸°í™”: {log_dir}")
    
    def log_metrics(self, metrics: Dict[str, Union[float, int]], step: Optional[int] = None):
        """ë©”íŠ¸ë¦­ì„ JSON Lines í˜•ì‹ìœ¼ë¡œ ì €ì¥"""
        log_entry = {"step": step, **metrics}
        
        with open(self.metrics_file, "a") as f:
            f.write(json.dumps(log_entry) + "\n")
        
        # ì½˜ì†” ì¶œë ¥
        step_str = f"Step {step}: " if step is not None else ""
        metric_str = ", ".join([f"{k}={v:.4f}" for k, v in metrics.items()])
        print(f"ğŸ“Š {step_str}{metric_str}")
    
    def log_config(self, config: Dict[str, Any]):
        """ì„¤ì • ì €ì¥"""
        with open(self.config_file, "w") as f:
            json.dump(config, f, indent=2)
    
    def save_plot(self, fig, filename: str):
        """í”Œë¡¯ ì €ì¥"""
        plot_path = self.log_dir / filename
        fig.savefig(plot_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“ˆ í”Œë¡¯ ì €ì¥: {plot_path}")

def create_experiment_summary(project_name: str, config: DictConfig, 
                            final_metrics: Dict[str, float]) -> str:
    """ì‹¤í—˜ ìš”ì•½ ìƒì„±"""
    import datetime
    
    summary = f"""
# ì‹¤í—˜ ìš”ì•½: {project_name}
ì‹¤í–‰ ì‹œê°„: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ì„¤ì •
- ëª¨ë¸: {config.model.backbone}
- ë°°ì¹˜ í¬ê¸°: {config.train.batch_size}
- í•™ìŠµë¥ : {config.train.lr}
- ì—í¬í¬: {config.train.epochs}
- ì¦ê°• ë ˆë²¨: {config.augmentation.level}

## ìµœì¢… ê²°ê³¼
"""
    
    for metric, value in final_metrics.items():
        summary += f"- {metric}: {value:.4f}\n"
    
    return summary

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª WandB ë¡œê±° í…ŒìŠ¤íŠ¸")
    
    # ê°€ì§œ ì„¤ì •
    from omegaconf import DictConfig
    
    test_config = DictConfig({
        "model": {"backbone": "resnet50", "num_classes": 396},
        "train": {"batch_size": 32, "lr": 0.001, "epochs": 10},
        "augmentation": {"level": "medium"}
    })
    
    # ë¡œê±° ì´ˆê¸°í™” (ì˜¤í”„ë¼ì¸ ëª¨ë“œ)
    logger = init_wandb(
        project_name="hecto-ai-test",
        config=test_config,
        experiment_name="test_experiment",
        tags=["test", "car_classification"],
        offline=True
    )
    
    # í…ŒìŠ¤íŠ¸ ë¡œê¹…
    if logger and logger.initialized:
        # ë©”íŠ¸ë¦­ ë¡œê¹…
        for epoch in range(3):
            metrics = {
                "train_loss": 2.5 - epoch * 0.3,
                "train_acc": 0.3 + epoch * 0.2,
                "val_loss": 2.3 - epoch * 0.25,
                "val_acc": 0.35 + epoch * 0.18
            }
            log_metrics(metrics, step=epoch)
        
        # ì‹œìŠ¤í…œ ì •ë³´ ë¡œê¹…
        logger.log_system_info()
        
        print("âœ… ë¡œê±° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # ì¢…ë£Œ
        finish_wandb()
    else:
        print("âš ï¸  ë¡œê±° ì´ˆê¸°í™” ì‹¤íŒ¨ - ë¡œì»¬ ë¡œê¹…ìœ¼ë¡œ ëŒ€ì²´")
        
        # ë¡œì»¬ ë¡œê±° í…ŒìŠ¤íŠ¸
        local_logger = LocalLogger("./test_logs")
        local_logger.log_metrics({"test_metric": 0.85}, step=1)
        local_logger.log_config(OmegaConf.to_container(test_config))
    
    print("ğŸ‰ ë¡œê±° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")